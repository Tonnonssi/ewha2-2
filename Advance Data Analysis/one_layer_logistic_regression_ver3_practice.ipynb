{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c098d5d0",
      "metadata": {
        "id": "c098d5d0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7d2fe3e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7d2fe3e",
        "outputId": "3cb9f881-746c-4634-d286-7c9e5c535dba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1000, 3)\n"
          ]
        }
      ],
      "source": [
        "# logistic regression\n",
        "\n",
        "num_sample = 500\n",
        "\n",
        "data0 = np.random.randn(num_sample,2) + (2,2) # np.random.randn() 함수는 표준 정규 분포에서 난수를 생성하는 함수\n",
        "                                              # (nrow=500,ncol=2)인 array의 각 row에 (2,2)를 더한다.\n",
        "data1 = np.random.randn(num_sample,2) + (-2,-2)\n",
        "\n",
        "data0 = np.hstack([data0,np.zeros((num_sample,1),dtype=float)]) # label = 0\n",
        "data1 = np.hstack([data1,np.ones((num_sample,1),dtype=float)])  # label = 1\n",
        "\n",
        "data = np.vstack([data0,data1])\n",
        "\n",
        "print(data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11002ec5",
      "metadata": {
        "id": "11002ec5"
      },
      "outputs": [],
      "source": [
        "#### HERE ####\n",
        "# we are assuming one layer logistic regression\n",
        "# layer = linear transformation + activation function\n",
        "zh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca4b0ade",
      "metadata": {
        "id": "ca4b0ade"
      },
      "outputs": [],
      "source": [
        "# define sigmoid function\n",
        "def sigmoid(val):\n",
        "    result = 1 / (1+np.exp(-val))\n",
        "    return result\n",
        "\n",
        "# define derivative of sigmoid function w.r.t. its value // w.r.t. : ~에 대해\n",
        "def grad_sigmoid(val):\n",
        "    result = sigmoid(val)*(1-sigmoid(val))\n",
        "    return result\n",
        "\n",
        "# given data instances in batch form,\n",
        "# compute loss and gradients of w and b\n",
        "# also, count the number of correct prediction\n",
        "\n",
        "def compute_loss_and_grad(data_instance): # 하나의 데이터가 들어감\n",
        "    '''\n",
        "    input : x=(x1,x2), y target\n",
        "    output : loss(target output vs predicted output), grad(w,b), hit\n",
        "    '''\n",
        "    x, y = data_instance\n",
        "\n",
        "    h = np.matmul(x,w) + b\n",
        "    pred = sigmoid(h) # pred\n",
        "    loss = -y*np.log(pred+delta)- (1-y)*np.log(1-pred+delta)\n",
        "    grad = -y*(1-sigmoid(h)) + (1-y)*sigmoid(h)\n",
        "\n",
        "    grad_w = np.multiply(grad,x)\n",
        "    grad_b = grad\n",
        "\n",
        "    hit = True if y == np.round(pred) else False # hit : 계산을 했을 때 맞췄는가 안 맞췄는가를 봄\n",
        "\n",
        "    return loss, (grad_w, grad_b), hit\n",
        "\n",
        "# update NN parameters w and b with SGD\n",
        "def update_parameters(params,grads):\n",
        "    w, b = params\n",
        "    grad_w, grad_b = grads\n",
        "\n",
        "    w -= eta*grad_w.reshape(-1,1) # -1 : 크기 알아서 잘 조절하삼 ㅇㅇ, 1 : 1차원 배열로 변환\n",
        "    b -= eta*grad_b\n",
        "\n",
        "    return w, b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9591e77c",
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9591e77c",
        "outputId": "3c53975c-486d-4499-c6a4-ef8c1cd60be1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 Train: 0.118 / 98.60 %\n",
            "Epoch 5 Train: 0.115 / 98.60 %\n",
            "Epoch 10 Train: 0.113 / 98.70 %\n",
            "Epoch 15 Train: 0.111 / 98.70 %\n",
            "Epoch 20 Train: 0.108 / 98.80 %\n",
            "Epoch 25 Train: 0.106 / 98.90 %\n",
            "Epoch 30 Train: 0.104 / 98.90 %\n",
            "Epoch 35 Train: 0.103 / 98.90 %\n",
            "Epoch 40 Train: 0.101 / 98.90 %\n",
            "Epoch 45 Train: 0.099 / 98.90 %\n",
            "Epoch 50 Train: 0.097 / 99.00 %\n",
            "Epoch 55 Train: 0.096 / 99.00 %\n",
            "Epoch 60 Train: 0.094 / 99.00 %\n",
            "Epoch 65 Train: 0.093 / 99.00 %\n",
            "Epoch 70 Train: 0.091 / 99.10 %\n",
            "Epoch 75 Train: 0.090 / 99.10 %\n",
            "Epoch 80 Train: 0.089 / 99.30 %\n",
            "Epoch 85 Train: 0.088 / 99.30 %\n",
            "Epoch 90 Train: 0.086 / 99.40 %\n",
            "Epoch 95 Train: 0.085 / 99.40 %\n"
          ]
        }
      ],
      "source": [
        "num_epoch = 100\n",
        "\n",
        "# train the logistic regression model\n",
        "for i in range(num_epoch):\n",
        "    # shuffle traning data by permutation\n",
        "    perm = np.random.permutation(len(data)) #난수 생성, 샘플을 뽑을 때 랜덤으로 뽑기 위해\n",
        "    total_loss = 0\n",
        "    count = 0\n",
        "\n",
        "    for j in range(len(data)):\n",
        "        # feed data instances one-by-one, i.e., mini-batch size is 1\n",
        "        x = data[perm[j]][:-1]\n",
        "        y = data[perm[j]][-1] # (1,1)\n",
        "        y = y.reshape([1])# (1,)\n",
        "        params = (w, b)\n",
        "        # compute loss and gradients, and then update the parameters\n",
        "        # also, compute sum of the loss and the number of correct prediction\n",
        "        loss, grads, hit = compute_loss_and_grad((x,y))\n",
        "        w, b = update_parameters(params, grads)\n",
        "        total_loss += loss.sum()\n",
        "        count += hit\n",
        "\n",
        "\n",
        "    #### HERE ####\n",
        "    # compute average loss and accuracy for the train dataset\n",
        "    loss_train = total_loss / len(data)\n",
        "    acc_train = count / len(data)\n",
        "\n",
        "    if i % 5 == 0:\n",
        "        print(\"Epoch %d Train: %.3f / %.2f %%\"%(i,loss_train,acc_train*100))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hBFwGExPzWh5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import MNIST, CIFAR10, CIFAR100\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = './datasets/'\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "train_data = MNIST(root=path, train=True, transform=transform, download=True)\n",
        "test_data = MNIST(root=path, train=False, transform=transform, download=True)\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "test_loader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "print(train_data)\n",
        "print(test_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCRRY2PDzjM5",
        "outputId": "3e38f259-3498-41f3-96ee-6aec4a192298"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./datasets/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 79557644.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./datasets/MNIST/raw/train-images-idx3-ubyte.gz to ./datasets/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./datasets/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 87273554.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./datasets/MNIST/raw/train-labels-idx1-ubyte.gz to ./datasets/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./datasets/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 27526483.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./datasets/MNIST/raw/t10k-images-idx3-ubyte.gz to ./datasets/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./datasets/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 14710833.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./datasets/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./datasets/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset MNIST\n",
            "    Number of datapoints: 60000\n",
            "    Root location: ./datasets/\n",
            "    Split: Train\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               ToTensor()\n",
            "           )\n",
            "Dataset MNIST\n",
            "    Number of datapoints: 10000\n",
            "    Root location: ./datasets/\n",
            "    Split: Test\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               ToTensor()\n",
            "           )\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_, seq_len, input_size = train_data[0][0].shape # (1,28,28)\n",
        "output_size = len(train_data.classes)"
      ],
      "metadata": {
        "id": "fUejBllAz-tP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "만약 데이터의 길이가 다른 경우 (MNIST는 데이터의 길이가 정해져 있음)\n",
        "-> rnn_padded_sepuence : 길이를 자동으로 변환해주는 함수"
      ],
      "metadata": {
        "id": "m5DsAVxy_pTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = input_size*2\n",
        "num_layers = 4\n",
        "batch_first = True\n",
        "bidirectional = True\n",
        "\n",
        "model_name = 'RNN'"
      ],
      "metadata": {
        "id": "8So_nKkB0R1u"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNClassifier(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers=1, batch_first=True, bidirectional=False):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.batch_first = batch_first\n",
        "        self.bidirectional = bidirectional\n",
        "\n",
        "        if self.bidirectional:\n",
        "            self.direction = 2\n",
        "        else:\n",
        "            self.direction = 1\n",
        "\n",
        "        self.seq = nn.RNN(input_size=self.input_size,\n",
        "                          hidden_size=self.hidden_size,\n",
        "                          num_layers=self.num_layers,\n",
        "                          batch_first=self.batch_first,\n",
        "                          bidirectional=self.bidirectional)\n",
        "        self.fc = nn.Linear(self.hidden_size*self.direction, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.reshape(-1, seq_len, self.input_size) # cell에서 있는 변환을 사용하지 않아도 된다.\n",
        "        hidden_state = torch.zeros(self.direction*self.num_layers,batch_size, self.hidden_size).to(device) # inital hidden을 세팅해준다. h_0\n",
        "        out, hidden = self.seq(x, hidden_state.detach().to(device)) # detach : nn는 그래프 구조로 이뤄져 있다. 이는 방향이 정해져 있음을 의미한다. detach는 역전파할 때 오류를 막는다.\n",
        "        # out, hidden : 최상단 층, 각각의 층에서의 last hidden\n",
        "        out = out[:,-1,:].squeeze() # 가장 우측 상단에 있는 hidden 데이터를 뽑아온다.\n",
        "        # squeeze는 필요없는 1 차원을 지운다.\n",
        "        out = self.fc(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "fL8hTCZczlSv"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if model_name == 'RNN':\n",
        "    classifier = RNNClassifier\n",
        "elif model_name == 'LSTM':\n",
        "    classifier = LSTMClassifier\n",
        "else:\n",
        "    classifier = GRUClassifier"
      ],
      "metadata": {
        "id": "LdlZxQB34CFd"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "Z9EiMtzF2MO5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = classifier(input_size, hidden_size, num_layers, batch_first, bidirectional).to(device)\n",
        "loss = nn.CrossEntropyLoss(reduction='sum')\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-3)"
      ],
      "metadata": {
        "id": "PowWfR5m4jzO"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epoch = 10\n",
        "train_loss_lst, test_loss_lst = list(), list()\n",
        "\n",
        "for i in range(num_epoch):\n",
        "    # training\n",
        "    model.train()\n",
        "\n",
        "    total_loss = 0\n",
        "    cnt = 0\n",
        "\n",
        "    for batch_idx, (x,y) in enumerate(train_loader):\n",
        "\n",
        "        x,y = x.to(device), y.to(device)\n",
        "        y_est = model.forward(x)\n",
        "        cost = loss(y_est, y)\n",
        "\n",
        "        total_loss += cost.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        cost.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        pred = torch.argmax(y_est, dim=1)\n",
        "        cnt += (pred == y).sum().item()\n",
        "\n",
        "    acc = cnt / len(train_data)\n",
        "    ave_loss = total_loss / len(train_data)\n",
        "\n",
        "    train_loss_lst.append(ave_loss)\n",
        "\n",
        "    if i % 1 == 0:\n",
        "        print(f\"\\nEpoch {i} Train : {ave_loss:.3f} / {acc:.3f}\")\n",
        "\n",
        "    #testing\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = 0\n",
        "    cnt = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch, (x,y) in enumerate(test_loader):\n",
        "\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            y_est = model.forward(x)\n",
        "            pred = torch.argmax(y_est, dim=1)\n",
        "\n",
        "            total_loss += cost.item()\n",
        "\n",
        "        acc = cnt / len(test_data)\n",
        "        ave_loss = total_loss / len(test_data)\n",
        "\n",
        "        test_loss_lst.append(ave_loss)\n",
        "\n",
        "        if i % 1 == 0:\n",
        "            print(f\"Epoch {i} Test : {ave_loss:.3f} / {acc:.3f}\")\n",
        "\n",
        "print()\n",
        "num_parameter = 0\n",
        "for parameter in model.parameters():\n",
        "    print(parameter.shape)\n",
        "    num_parameter += np.prod(parameter.size())\n",
        "print(num_parameter)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qNCbCetsCM2",
        "outputId": "72961265-ebd4-4b91-8bf0-29d761d130e1"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 0 Train : 0.761 / 0.741\n",
            "Epoch 0 Test : 0.426 / 0.000\n",
            "\n",
            "Epoch 1 Train : 0.276 / 0.914\n",
            "Epoch 1 Test : 0.096 / 0.000\n",
            "\n",
            "Epoch 2 Train : 0.202 / 0.938\n",
            "Epoch 2 Test : 0.246 / 0.000\n",
            "\n",
            "Epoch 3 Train : 0.164 / 0.951\n",
            "Epoch 3 Test : 0.166 / 0.000\n",
            "\n",
            "Epoch 4 Train : 0.145 / 0.957\n",
            "Epoch 4 Test : 0.066 / 0.000\n",
            "\n",
            "Epoch 5 Train : 0.128 / 0.961\n",
            "Epoch 5 Test : 0.104 / 0.000\n",
            "\n",
            "Epoch 6 Train : 0.109 / 0.968\n",
            "Epoch 6 Test : 0.088 / 0.000\n",
            "\n",
            "Epoch 7 Train : 0.109 / 0.967\n",
            "Epoch 7 Test : 0.076 / 0.000\n",
            "\n",
            "Epoch 8 Train : 0.101 / 0.970\n",
            "Epoch 8 Test : 0.059 / 0.000\n",
            "\n",
            "Epoch 9 Train : 0.095 / 0.972\n",
            "Epoch 9 Test : 0.139 / 0.000\n",
            "\n",
            "torch.Size([56, 28])\n",
            "torch.Size([56, 56])\n",
            "torch.Size([56])\n",
            "torch.Size([56])\n",
            "torch.Size([56, 28])\n",
            "torch.Size([56, 56])\n",
            "torch.Size([56])\n",
            "torch.Size([56])\n",
            "torch.Size([56, 112])\n",
            "torch.Size([56, 56])\n",
            "torch.Size([56])\n",
            "torch.Size([56])\n",
            "torch.Size([56, 112])\n",
            "torch.Size([56, 56])\n",
            "torch.Size([56])\n",
            "torch.Size([56])\n",
            "torch.Size([56, 112])\n",
            "torch.Size([56, 56])\n",
            "torch.Size([56])\n",
            "torch.Size([56])\n",
            "torch.Size([56, 112])\n",
            "torch.Size([56, 56])\n",
            "torch.Size([56])\n",
            "torch.Size([56])\n",
            "torch.Size([56, 112])\n",
            "torch.Size([56, 56])\n",
            "torch.Size([56])\n",
            "torch.Size([56])\n",
            "torch.Size([56, 112])\n",
            "torch.Size([56, 56])\n",
            "torch.Size([56])\n",
            "torch.Size([56])\n",
            "torch.Size([10, 112])\n",
            "torch.Size([10])\n",
            "67882\n"
          ]
        }
      ]
    }
  ]
}
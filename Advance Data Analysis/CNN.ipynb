{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bUcYuuwmfG_l"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import MNIST, CIFAR10, CIFAR100\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = './datasets/'\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "train_data = MNIST(root=path, train=True, transform=transform, download=True)\n",
        "test_data = MNIST(root=path, train=False, transform=transform, download=True)\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "test_loader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "print(train_data)\n",
        "print(test_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yhOGv7VlUol",
        "outputId": "52ac886a-a666-45c2-802b-6e1d12e59dab"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./datasets/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 183724319.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./datasets/MNIST/raw/train-images-idx3-ubyte.gz to ./datasets/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./datasets/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 34219122.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./datasets/MNIST/raw/train-labels-idx1-ubyte.gz to ./datasets/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./datasets/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 40146117.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./datasets/MNIST/raw/t10k-images-idx3-ubyte.gz to ./datasets/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./datasets/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 4410865.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./datasets/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./datasets/MNIST/raw\n",
            "\n",
            "Dataset MNIST\n",
            "    Number of datapoints: 60000\n",
            "    Root location: ./datasets/\n",
            "    Split: Train\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               ToTensor()\n",
            "           )\n",
            "Dataset MNIST\n",
            "    Number of datapoints: 10000\n",
            "    Root location: ./datasets/\n",
            "    Split: Test\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               ToTensor()\n",
            "           )\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.classes # check the classes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Gm4-t9flgOt",
        "outputId": "745008b5-965b-4252-c1a0-935729248859"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['0 - zero',\n",
              " '1 - one',\n",
              " '2 - two',\n",
              " '3 - three',\n",
              " '4 - four',\n",
              " '5 - five',\n",
              " '6 - six',\n",
              " '7 - seven',\n",
              " '8 - eight',\n",
              " '9 - nine']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = train_data[0][0].shape\n",
        "output_shape = len(train_data.classes)"
      ],
      "metadata": {
        "id": "3-iGAGdClkBL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=(5,5), stride=1, padding=2)\n",
        "        self.pool1 = nn.AvgPool2d(kernel_size=(2,2), stride=2, padding=0)\n",
        "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=(5,5), stride=1)\n",
        "        self.pool2 = nn.AvgPool2d(kernel_size=(2,2), stride=2, padding=0)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(400, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, output_shape)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.leaky_relu(self.conv1(x)) # shape : 100x6x28x28 print(x.shape)\n",
        "        x = self.pool1(x) # shape : 100x6x14x14\n",
        "        x = F.leaky_relu(self.conv2(x)) # shape : 100x16x10x10\n",
        "        x = self.pool2(x) # shape : 100x16x5x5\n",
        "        x = self.flatten(x)\n",
        "        x = F.leaky_relu(self.fc1(x)) # shape : 400 -> 120\n",
        "        x = F.leaky_relu(self.fc2(x)) # shape : 120 -> 84\n",
        "        x = self.fc3(x) # shape : 84 -> 10\n",
        "        return x"
      ],
      "metadata": {
        "id": "HekXTdw2mCyJ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "lhIS7rLKqU7v"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LeNet().to(device)\n",
        "loss = nn.CrossEntropyLoss(reduction='sum')\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-3)"
      ],
      "metadata": {
        "id": "DzunhyUPqo6X"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epoch = 10\n",
        "train_loss_lst, test_loss_lst = list(), list()\n",
        "\n",
        "for i in range(num_epoch):\n",
        "    # training\n",
        "    model.train()\n",
        "\n",
        "    total_loss = 0\n",
        "    cnt = 0\n",
        "\n",
        "    for batch_idx, (x,y) in enumerate(train_loader):\n",
        "\n",
        "        x,y = x.to(device), y.to(device)\n",
        "        y_est = model.forward(x)\n",
        "        cost = loss(y_est, y)\n",
        "\n",
        "        total_loss += cost.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        cost.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        pred = torch.argmax(y_est, dim=1)\n",
        "        cnt += (pred == y).sum().item()\n",
        "\n",
        "    acc = cnt / len(train_data)\n",
        "    ave_loss = total_loss / len(train_data)\n",
        "\n",
        "    train_loss_lst.append(ave_loss)\n",
        "\n",
        "    if i % 1 == 0:\n",
        "        print(f\"\\nEpoch {i} Train : {ave_loss:.3f} / {acc:.3f}\")\n",
        "\n",
        "    #testing\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = 0\n",
        "    cnt = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch, (x,y) in enumerate(test_loader):\n",
        "\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            y_est = model.forward(x)\n",
        "            pred = torch.argmax(y_est, dim=1)\n",
        "\n",
        "            total_loss += cost.item()\n",
        "\n",
        "        acc = cnt / len(test_data)\n",
        "        ave_loss = total_loss / len(test_data)\n",
        "\n",
        "        test_loss_lst.append(ave_loss)\n",
        "\n",
        "        if i % 1 == 0:\n",
        "            print(f\"Epoch {i} Test : {ave_loss:.3f} / {acc:.3f}\")\n",
        "\n",
        "print()\n",
        "num_parameter = 0\n",
        "for parameter in model.parameters():\n",
        "    print(parameter.shape)\n",
        "    num_parameter += np.prod(parameter.size())\n",
        "print(num_parameter)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qNCbCetsCM2",
        "outputId": "5466d039-09fa-4f29-eef6-0976d8bedb58"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 0 Train : 0.401 / 0.877\n",
            "Epoch 0 Test : 0.161 / 0.000\n",
            "\n",
            "Epoch 1 Train : 0.107 / 0.968\n",
            "Epoch 1 Test : 0.045 / 0.000\n",
            "\n",
            "Epoch 2 Train : 0.073 / 0.977\n",
            "Epoch 2 Test : 0.035 / 0.000\n",
            "\n",
            "Epoch 3 Train : 0.058 / 0.983\n",
            "Epoch 3 Test : 0.053 / 0.000\n",
            "\n",
            "Epoch 4 Train : 0.051 / 0.984\n",
            "Epoch 4 Test : 0.077 / 0.000\n",
            "\n",
            "Epoch 5 Train : 0.042 / 0.987\n",
            "Epoch 5 Test : 0.016 / 0.000\n",
            "\n",
            "Epoch 6 Train : 0.037 / 0.988\n",
            "Epoch 6 Test : 0.014 / 0.000\n",
            "\n",
            "Epoch 7 Train : 0.033 / 0.989\n",
            "Epoch 7 Test : 0.025 / 0.000\n",
            "\n",
            "Epoch 8 Train : 0.029 / 0.991\n",
            "Epoch 8 Test : 0.007 / 0.000\n",
            "\n",
            "Epoch 9 Train : 0.025 / 0.992\n",
            "Epoch 9 Test : 0.044 / 0.000\n",
            "\n",
            "torch.Size([6, 1, 5, 5])\n",
            "torch.Size([6])\n",
            "torch.Size([16, 6, 5, 5])\n",
            "torch.Size([16])\n",
            "torch.Size([120, 400])\n",
            "torch.Size([120])\n",
            "torch.Size([84, 120])\n",
            "torch.Size([84])\n",
            "torch.Size([10, 84])\n",
            "torch.Size([10])\n",
            "61706\n"
          ]
        }
      ]
    }
  ]
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35f3801b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bcb0520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 3])\n"
     ]
    }
   ],
   "source": [
    "# Making Dataset\n",
    "\n",
    "num_sample = 250 \n",
    "\n",
    "# XOR Problem \n",
    "data0 = np.random.randn(num_sample, 2) + (2,2)\n",
    "data1 = np.random.randn(num_sample, 2) + (-2,-2)\n",
    "data2 = np.random.randn(num_sample, 2) + (2,-2)\n",
    "data3 = np.random.randn(num_sample, 2) + (-2,2)\n",
    "\n",
    "# labeling \n",
    "data0 = np.hstack([data0, np.zeros((num_sample, 1), dtype=float)])\n",
    "data1 = np.hstack([data1, np.zeros((num_sample, 1), dtype=float)])\n",
    "data2 = np.hstack([data2, np.ones((num_sample, 1), dtype=float)])\n",
    "data3 = np.hstack([data3, np.ones((num_sample, 1), dtype=float)])\n",
    "\n",
    "# make into single dataset\n",
    "data = np.vstack([data0, data1, data2, data3])\n",
    "data = torch.tensor(data, dtype=torch.float32)\n",
    "\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a3f6c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(2,3)\n",
    "        self.linear2 = nn.Linear(3,3)\n",
    "        self.linear3 = nn.Linear(3,1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.linear1(x)) # hidden\n",
    "        x = F.tanh(self.linear2(x)) # hidden \n",
    "        x = F.sigmoid(self.linear3(x)) # output\n",
    "        # x = self.linear3(x) # logit : inverse of sigmoid(softmax)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "baeff5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "loss = nn.BCELoss() # need sigmoid activation\n",
    "# loss = nn.BCEWithLogitsLoss : do not need sigmoid, logit만 있으면 됨 \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ddc5096",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 200\n",
    "loss_lst = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7ef64b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 257.6317257359624 / 0.919\n",
      "Epoch 10: 124.21806702786125 / 0.955\n",
      "Epoch 20: 122.2580069063697 / 0.958\n",
      "Epoch 30: 120.73169162636623 / 0.958\n",
      "Epoch 40: 117.7627717081923 / 0.958\n",
      "Epoch 50: 117.65909820585512 / 0.961\n",
      "Epoch 60: 115.21672062086873 / 0.964\n",
      "Epoch 70: 110.96346937050112 / 0.963\n",
      "Epoch 80: 110.51253686391283 / 0.959\n",
      "Epoch 90: 109.14128299325239 / 0.966\n",
      "Epoch 100: 103.72953682625666 / 0.967\n",
      "Epoch 110: 110.09194405714516 / 0.96\n",
      "Epoch 120: 104.2374648269033 / 0.964\n",
      "Epoch 130: 111.34838612168096 / 0.964\n",
      "Epoch 140: 103.63615949382074 / 0.963\n",
      "Epoch 150: 102.4415699786041 / 0.958\n",
      "Epoch 160: 106.58613431686535 / 0.96\n",
      "Epoch 170: 98.74705228558742 / 0.963\n",
      "Epoch 180: 100.82493243878707 / 0.964\n",
      "Epoch 190: 97.07426498946734 / 0.965\n"
     ]
    }
   ],
   "source": [
    "for i in range(EPOCHS):\n",
    "    shuffled_dataset = np.random.permutation(data)\n",
    "    shuffled_dataset = torch.tensor(shuffled_dataset)\n",
    "    total_loss = 0\n",
    "    cnt = 0\n",
    "    \n",
    "    for j in range(len(shuffled_dataset)):\n",
    "        x = shuffled_dataset[j][:-1]\n",
    "        y = shuffled_dataset[j][-1]\n",
    "        \n",
    "        y = y.reshape([1]) # torch.Size([1])\n",
    "        pred = model.forward(x) # output = P(y=1|x)\n",
    "        # logit = mpdel.forward(x)\n",
    "        # prob = F.sigmoid(logit)\n",
    "        \n",
    "        cost = loss(pred, y) # cal loss\n",
    "        # cost = loss(logit, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += cost.item()\n",
    "        \n",
    "        pred = torch.round(pred).item()\n",
    "        cnt += (pred == y.item())\n",
    "        \n",
    "    acc = cnt / len(shuffled_dataset)\n",
    "    loss_lst.append(total_loss)\n",
    "    \n",
    "    if i % 10 ==0:\n",
    "        print(f\"Epoch {i}: {total_loss} / {acc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

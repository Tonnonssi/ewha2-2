{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hBFwGExPzWh5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import MNIST, CIFAR10, CIFAR100\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = './datasets/'\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "train_data = MNIST(root=path, train=True, transform=transform, download=True)\n",
        "test_data = MNIST(root=path, train=False, transform=transform, download=True)\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "test_loader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "print(train_data)\n",
        "print(test_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCRRY2PDzjM5",
        "outputId": "2aca3b54-caef-4b26-e469-fc16b537767f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./datasets/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 74085571.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./datasets/MNIST/raw/train-images-idx3-ubyte.gz to ./datasets/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./datasets/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 78813073.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./datasets/MNIST/raw/train-labels-idx1-ubyte.gz to ./datasets/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./datasets/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 28128587.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./datasets/MNIST/raw/t10k-images-idx3-ubyte.gz to ./datasets/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./datasets/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 3245958.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./datasets/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./datasets/MNIST/raw\n",
            "\n",
            "Dataset MNIST\n",
            "    Number of datapoints: 60000\n",
            "    Root location: ./datasets/\n",
            "    Split: Train\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               ToTensor()\n",
            "           )\n",
            "Dataset MNIST\n",
            "    Number of datapoints: 10000\n",
            "    Root location: ./datasets/\n",
            "    Split: Test\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               ToTensor()\n",
            "           )\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_, seq_len, input_size = train_data[0][0].shape # (1,28,28)\n",
        "output_size = len(train_data.classes)"
      ],
      "metadata": {
        "id": "fUejBllAz-tP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "Z9EiMtzF2MO5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = input_size*2\n",
        "model_name = 'LSTM'"
      ],
      "metadata": {
        "id": "8So_nKkB0R1u"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNClassifier(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.cell = nn.RNNCell(input_size=self.input_size,\n",
        "                               hidden_size=self.hidden_size)\n",
        "        self.fc = nn.Linear(self.hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.reshape(-1, seq_len, self.input_size).permute((1,0,2)) # transpose 28,100,28\n",
        "         # img data라서 필요한 부분, 100x1x28x28 -> 100x28x28\n",
        "        hidden_state = torch.zeros(batch_size, self.hidden_size).to(device) # inital hidden을 세팅해준다. h_0\n",
        "        for i in range(seq_len):\n",
        "            hidden_state = self.cell(x[i], hidden_state)\n",
        "        out = self.fc(hidden_state)\n",
        "        return out"
      ],
      "metadata": {
        "id": "fL8hTCZczlSv"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.cell = nn.LSTMCell(input_size=self.input_size,\n",
        "                               hidden_size=self.hidden_size)\n",
        "        self.fc = nn.Linear(self.hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.reshape(-1, seq_len, self.input_size).permute((1,0,2))\n",
        "        hidden_state = torch.zeros(batch_size, self.hidden_size).to(device)\n",
        "        cell_state = torch.zeros(batch_size, self.hidden_size).to(device)\n",
        "        for i in range(seq_len):\n",
        "            hidden_state, cell_state = self.cell(x[i], (hidden_state, cell_state))\n",
        "        out = self.fc(hidden_state)\n",
        "        return out"
      ],
      "metadata": {
        "id": "o54jkSJZ19J0"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GRUClassifier(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.cell = nn.GRUCell(input_size=self.input_size,\n",
        "                               hidden_size=self.hidden_size)\n",
        "        self.fc = nn.Linear(self.hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.reshape(-1, seq_len, self.input_size).permute((1,0,2))\n",
        "        hidden_state = torch.zeros(batch_size, self.hidden_size).to(device)\n",
        "        for i in range(seq_len):\n",
        "            hidden_state = self.cell(x[i], hidden_state)\n",
        "        out = self.fc(hidden_state)\n",
        "        return out"
      ],
      "metadata": {
        "id": "2hm5YxzN3y-z"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if model_name == 'RNN':\n",
        "    classifier = RNNClassifier\n",
        "elif model_name == 'LSTM':\n",
        "    classifier = LSTMClassifier\n",
        "else:\n",
        "    classifier = GRUClassifier"
      ],
      "metadata": {
        "id": "LdlZxQB34CFd"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = classifier(input_size, hidden_size).to(device)\n",
        "loss = nn.CrossEntropyLoss(reduction='sum')\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-3)"
      ],
      "metadata": {
        "id": "PowWfR5m4jzO"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epoch = 10\n",
        "train_loss_lst, test_loss_lst = list(), list()\n",
        "\n",
        "for i in range(num_epoch):\n",
        "    # training\n",
        "    model.train()\n",
        "\n",
        "    total_loss = 0\n",
        "    cnt = 0\n",
        "\n",
        "    for batch_idx, (x,y) in enumerate(train_loader):\n",
        "\n",
        "        x,y = x.to(device), y.to(device)\n",
        "        y_est = model.forward(x)\n",
        "        cost = loss(y_est, y)\n",
        "\n",
        "        total_loss += cost.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        cost.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        pred = torch.argmax(y_est, dim=1)\n",
        "        cnt += (pred == y).sum().item()\n",
        "\n",
        "    acc = cnt / len(train_data)\n",
        "    ave_loss = total_loss / len(train_data)\n",
        "\n",
        "    train_loss_lst.append(ave_loss)\n",
        "\n",
        "    if i % 1 == 0:\n",
        "        print(f\"\\nEpoch {i} Train : {ave_loss:.3f} / {acc:.3f}\")\n",
        "\n",
        "    #testing\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = 0\n",
        "    cnt = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch, (x,y) in enumerate(test_loader):\n",
        "\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            y_est = model.forward(x)\n",
        "            pred = torch.argmax(y_est, dim=1)\n",
        "\n",
        "            total_loss += cost.item()\n",
        "\n",
        "        acc = cnt / len(test_data)\n",
        "        ave_loss = total_loss / len(test_data)\n",
        "\n",
        "        test_loss_lst.append(ave_loss)\n",
        "\n",
        "        if i % 1 == 0:\n",
        "            print(f\"Epoch {i} Test : {ave_loss:.3f} / {acc:.3f}\")\n",
        "\n",
        "print()\n",
        "num_parameter = 0\n",
        "for parameter in model.parameters():\n",
        "    print(parameter.shape)\n",
        "    num_parameter += np.prod(parameter.size())\n",
        "print(num_parameter)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qNCbCetsCM2",
        "outputId": "b961d650-b0a0-42ac-fe3d-a7156fee867a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 0 Train : 0.748 / 0.758\n",
            "Epoch 0 Test : 0.345 / 0.000\n",
            "\n",
            "Epoch 1 Train : 0.230 / 0.933\n",
            "Epoch 1 Test : 0.098 / 0.000\n",
            "\n",
            "Epoch 2 Train : 0.160 / 0.953\n",
            "Epoch 2 Test : 0.129 / 0.000\n",
            "\n",
            "Epoch 3 Train : 0.127 / 0.963\n",
            "Epoch 3 Test : 0.083 / 0.000\n",
            "\n",
            "Epoch 4 Train : 0.109 / 0.968\n",
            "Epoch 4 Test : 0.049 / 0.000\n",
            "\n",
            "Epoch 5 Train : 0.095 / 0.971\n",
            "Epoch 5 Test : 0.097 / 0.000\n",
            "\n",
            "Epoch 6 Train : 0.084 / 0.975\n",
            "Epoch 6 Test : 0.047 / 0.000\n",
            "\n",
            "Epoch 7 Train : 0.075 / 0.977\n",
            "Epoch 7 Test : 0.069 / 0.000\n",
            "\n",
            "Epoch 8 Train : 0.069 / 0.979\n",
            "Epoch 8 Test : 0.039 / 0.000\n",
            "\n",
            "Epoch 9 Train : 0.062 / 0.981\n",
            "Epoch 9 Test : 0.028 / 0.000\n",
            "\n",
            "torch.Size([224, 28])\n",
            "torch.Size([224, 56])\n",
            "torch.Size([224])\n",
            "torch.Size([224])\n",
            "torch.Size([10, 56])\n",
            "torch.Size([10])\n",
            "19834\n"
          ]
        }
      ]
    }
  ]
}